import boto3
import pandas as pd
import os

# Initialize S3 client
s3 = boto3.client('s3')

# Define source and destination buckets
source_bucket = 'agco-servicepublications-agentforce-dev'
destination_bucket = 'agco-servicepublications-agentforce-dev'

# Define the base path where source files are stored
source_base_path = "knowledge/agco/"  # Adjusted to match actual S3 path

# Load the CSV file into a DataFrame
df = pd.read_csv('/home/cloudshell-user/TestScript/metadata.csv', sep=',', quotechar='"')

# Select necessary columns
df = df[['Type', 'filename', 'brandcode', 'machinetypecode']].copy()

# Clean up the data
df['machinetypecode'] = df['machinetypecode'].astype(str).str.replace(',', '-').str.replace(' ', '')
df['brandcode'] = df['brandcode'].astype(str).str.replace(',', '-').str.replace(' ', '')
df['Type'] = df['Type'].astype(str).str.upper()

# Define log file
log_file = "failed_transfers.log"

# Function to log failures
def log_failure(message):
    with open(log_file, "a") as log:
        log.write(message + "\n")

# Paginate through source bucket to find files inside the nested path
paginator = s3.get_paginator('list_objects_v2')

# Iterate over each row in the DataFrame
for index, row in df.iterrows():
    file_type = row['Type']  # Identify if it's OM or WSM
    filename = row['filename']
    brandcode = row['brandcode']
    machinetypecode = row['machinetypecode']

    # Ensure the type is either OM or WSM, else default to "OM other" or "WSM other"
    if file_type in ["OM", "WSM"]:
        folder_name = f"aws s3 service pubs {file_type} en_GB {brandcode} {machinetypecode}"
    else:
        folder_name = f"aws s3 service pubs {file_type} en_GB other"

    # Search inside the nested path
    search_prefix = f"{source_base_path}{file_type}/en-GB/"
    file_found = False

    page_iterator = paginator.paginate(Bucket=source_bucket, Prefix=search_prefix)

    for page in page_iterator:
        for obj in page.get('Contents', []):
            if obj['Key'].endswith(filename):
                source_key = obj['Key']
                destination_key = f"{folder_name}/{filename}"
                print(f"Source Key: {source_key}")
                print(f"Destination Key: {destination_key}")

                try:
                    # Copy the file
                    copy_source = {'Bucket': source_bucket, 'Key': source_key}
                    response = s3.copy_object(Bucket=destination_bucket, CopySource=copy_source, Key=destination_key)

                    if response['ResponseMetadata']['HTTPStatusCode'] == 200:
                        print(f"Successfully copied {source_key} to {destination_key}")
                    else:
                        print(f"Failed to copy {source_key} to {destination_key}")
                        log_failure(f"Failed to copy {source_key} to {destination_key}")

                except Exception as e:
                    print(f"Error copying {source_key} to {destination_key}: {e}")
                    log_failure(f"Error copying {source_key} to {destination_key}: {e}")

                file_found = True
                break
        if file_found:
            break

    # If file not found, move it to the respective "other" folder
    if not file_found:
        other_folder = f"aws s3 service pubs {file_type} en_GB other"
        destination_key = f"{other_folder}/{filename}"
        print(f"File {filename} not found in expected location. Moving to {destination_key}")

print("File transfer process completed.")