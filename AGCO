import boto3
import pandas as pd
import os
import concurrent.futures

# AWS S3 Client
s3 = boto3.client('s3')

# Source and Destination S3 Buckets
source_bucket = 'agco-servicepublications-agentforce-dev'
destination_bucket = 'agco-servicepublications-agentforce-dev'

# Hardcoded Destination Folder Mapping for WSM
destination_folders_wsm = {
    "FT 100": "AWS S3 Service Pubs WSM en_GB FT 100",
    "FT 200": "AWS S3 Service Pubs WSM en_GB FT 200",
    "FT 400": "AWS S3 Service Pubs WSM en_GB FT 400",
    "FT 500": "AWS S3 Service Pubs WSM en_GB FT 500",
    "MF 100": "AWS S3 Service Pubs WSM en_GB MF 100",
    "MF 200": "AWS S3 Service Pubs WSM en_GB MF 200",
    "MF 400": "AWS S3 Service Pubs WSM en_GB MF 400",
    "MF 500": "AWS S3 Service Pubs WSM en_GB MF 500",
    "CH 100": "AWS S3 Service Pubs WSM en_GB CH 100",
    "CH 200": "AWS S3 Service Pubs WSM en_GB CH 200",
    "CH 400": "AWS S3 Service Pubs WSM en_GB CH 400",
    "CH 500": "AWS S3 Service Pubs WSM en_GB CH 500",
    "VT 100": "AWS S3 Service Pubs WSM en_GB VT 100",
    "VT 200": "AWS S3 Service Pubs WSM en_GB VT 200",
    "VT 400": "AWS S3 Service Pubs WSM en_GB VT 400",
    "VT 500": "AWS S3 Service Pubs WSM en_GB VT 500",
    "Other": "AWS S3 Service Pubs WSM en_GB Other",
}

# Hardcoded Destination Folder Mapping for OM (Matching WSM Folder Names)
destination_folders_om = {
    "FT 100": "AWS S3 Service Pubs OM en_GB FT 100",
    "FT 200": "AWS S3 Service Pubs OM en_GB FT 200",
    "FT 400": "AWS S3 Service Pubs OM en_GB FT 400",
    "FT 500": "AWS S3 Service Pubs OM en_GB FT 500",
    "MF 100": "AWS S3 Service Pubs OM en_GB MF 100",
    "MF 200": "AWS S3 Service Pubs OM en_GB MF 200",
    "MF 400": "AWS S3 Service Pubs OM en_GB MF 400",
    "MF 500": "AWS S3 Service Pubs OM en_GB MF 500",
    "CH 100": "AWS S3 Service Pubs OM en_GB CH 100",
    "CH 200": "AWS S3 Service Pubs OM en_GB CH 200",
    "CH 400": "AWS S3 Service Pubs OM en_GB CH 400",
    "CH 500": "AWS S3 Service Pubs OM en_GB CH 500",
    "VT 100": "AWS S3 Service Pubs OM en_GB VT 100",
    "VT 200": "AWS S3 Service Pubs OM en_GB VT 200",
    "VT 400": "AWS S3 Service Pubs OM en_GB VT 400",
    "VT 500": "AWS S3 Service Pubs OM en_GB VT 500",
    "Other": "AWS S3 Service Pubs OM en_GB Other",
}

# Multipart Upload Threshold (100MB)
MULTIPART_THRESHOLD = 100 * 1024 * 1024  # 100MB


def get_destination_folders(brand_code, machine_type_code, destination_folders):
    """Determine correct destination folder based on Brand Code & Machine Type Code."""
    matching_folders = []

    if brand_code and machine_type_code:
        combined_code = f"{brand_code} {machine_type_code}"
        if combined_code in destination_folders:
            matching_folders.append(destination_folders[combined_code])

    if not matching_folders:
        matching_folders.append(destination_folders["Other"])

    return matching_folders


def copy_file(source_key, destination_key):
    """Copies file to destination using either standard or multipart upload."""
    try:
        obj = s3.head_object(Bucket=source_bucket, Key=source_key)
        file_size = obj['ContentLength']

        if file_size > MULTIPART_THRESHOLD:
            multipart_upload(source_key, destination_key)
        else:
            s3.copy_object(
                Bucket=destination_bucket,
                CopySource={'Bucket': source_bucket, 'Key': source_key},
                Key=destination_key
            )
            print(f"Copied: {source_key} → {destination_key}")

    except Exception as e:
        print(f"Error copying {source_key} → {destination_key}: {e}")


def process_file(part_number, brand_code, machine_type_code, destination_folders):
    """Finds and copies files from S3."""
    folders = get_destination_folders(brand_code, machine_type_code, destination_folders)

    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=source_bucket, Prefix=f"{part_number}/")

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_key = {}

        for page in page_iterator:
            for obj in page.get('Contents', []):
                source_key = obj['Key']
                filename = os.path.basename(source_key)

                for folder in folders:
                    destination_key = f"{folder}/{filename}"
                    future = executor.submit(copy_file, source_key, destination_key)
                    future_to_key[future] = source_key

        for future in concurrent.futures.as_completed(future_to_key):
            future.result()


metadata_files = {
    "WSM": ("WSM.csv", destination_folders_wsm),
    "OM": ("OM.csv", destination_folders_om)
}

for folder, (metadata_file, destination_folders) in metadata_files.items():
    df = pd.read_csv(metadata_file, sep=',', quotechar='"')

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_tasks = {
            executor.submit(process_file, row['Part Number'], row['BrandCode'], row['MachineTypeCode'], destination_folders): row['Part Number']
            for _, row in df.iterrows()
        }

        for future in concurrent.futures.as_completed(future_tasks):
            future.result()