import boto3
import pandas as pd
import logging
import botocore
from concurrent.futures import ThreadPoolExecutor, as_completed
from boto3.s3.transfer import TransferConfig
from time import sleep

# Updated Global Variables for S3 Buckets (Based on Your Script)
SOURCE_BUCKET = "agco-servicepublications-agentforce-dev"
DESTINATION_BUCKET = "agco-servicepublications-agentforce-dev"

# Define Source & Destination Parent Folders
SOURCE_PARENT_FOLDER = "knowledge/agco/WSM-en-GB/"
DESTINATION_PARENT_FOLDER = "knowledge/agco/WSM-Target_SplitFiles/"

# Setup logging
logging.basicConfig(filename="s3_transfer.log", level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")

s3 = boto3.client("s3", config=botocore.config.Config(retries={"max_attempts": 10}))

# Multipart Upload Configuration
config = TransferConfig(
    multipart_threshold=8 * 1024 * 1024,  # 8MB threshold
    max_concurrency=20,  # Adaptive concurrency
    multipart_chunksize=16 * 1024 * 1024,  # 16MB per part
    use_threads=True
)

def transfer_file(row):
    """Copies a file from source to destination bucket based on metadata conditions."""
    if not row or "filename" not in row:
        logging.error("Invalid row data: row is None or missing filename.")
        return

    filename = row.get("filename", "unknown.txt")
    brandcode = row.get("brandcode", "Unknown") or "Unknown"
    machinetypecode = row.get("machinetypecode", "Unknown") or "Unknown"
    file_type = row.get("type", "WSM")

    source_key = f"{SOURCE_PARENT_FOLDER}{filename}"
    
    # Check if the source file exists before attempting to copy
    try:
        s3.head_object(Bucket=SOURCE_BUCKET, Key=source_key)
    except botocore.exceptions.ClientError as e:
        if e.response['Error']['Code'] == "404":
            logging.error(f"Source file does not exist: {source_key}")
            return  # Skip copying if file is missing
        logging.error(f"Error checking source file: {e}")
        return  # Exit function if unknown error

    # Handle missing metadata
    if brandcode == "Unknown" or machinetypecode == "Unknown":
        destination_key = f"{DESTINATION_PARENT_FOLDER}Other/{filename}"
    else:
        destination_key = f"{DESTINATION_PARENT_FOLDER}AWS S3 Service Pubs {file_type} en-GB {brandcode} {machinetypecode}/{filename}"

    for attempt in range(5):  # Retry up to 5 times
        try:
            copy_source = {"Bucket": SOURCE_BUCKET, "Key": source_key}
            response = s3.copy(copy_source, DESTINATION_BUCKET, destination_key)

            # Fix: Check if response is None before accessing it
            if response is None:
                logging.warning(f"Copy response is None for {source_key}, retrying...")
                continue  # Retry the operation
            
            if "ResponseMetadata" in response and response["ResponseMetadata"].get("HTTPStatusCode") == 200:
                logging.info(f"Successfully copied {source_key} to {destination_key}")
                return
            else:
                logging.warning(f"Failed to copy {source_key} to {destination_key}, retrying...")

        except botocore.exceptions.ClientError as e:
            logging.error(f"Error copying {source_key}: {e}")

        sleep(5)  # Wait before retrying

    logging.error(f"Failed to copy {source_key} after multiple attempts.")

def process_files(csv_file):
    """Reads metadata CSV and processes file transfers in parallel."""
    df = pd.read_csv(csv_file)
    df.fillna("Unknown", inplace=True)  # Replace NaN with 'Unknown'

    MAX_WORKERS = 20  # Adaptive concurrency

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(transfer_file, row): row for _, row in df.iterrows()}
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                logging.error(f"Unexpected error: {e}")

if __name__ == "__main__":
    process_files("/home/cloudshell-user/TestScript/metadata.csv")